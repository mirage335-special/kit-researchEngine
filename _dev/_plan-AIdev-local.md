
Axolotl Docker container installed on a Cygwin/MSW platform .
```bash
#--privileged
#--ipc=host --ulimit memlock=-1 --ulimit stack=67108864
docker run --shm-size=20g --name axolotl-$(_uid 14) --gpus "all" -v 'C:\q':/q -v 'C:\core':/core -v "$USERPROFILE"'\Downloads':/Downloads -v "$factory_outputsDir":/outputs -v "$factory_modelDir":/model -v "$factory_datasetsDir":/datasets -v "$factory_projectDir":/workspace/project --rm -it axolotlai/axolotl:main-latest
```


Models (eg. Meta-Llama-3.1-8B-Instruct-abliterated , Llama-3.2-1B ) in safetensors format available at: 'C:\q\p\zFactory\Llama-tech\models' or similar .
Outputs (eg. LORA adapters) saved at: 'C:\q\p\zFactory\Llama-tech\outputs' or similar .

Datasets (eg. JSONL prompt/response fine tuning) available at: 'C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset' or 'C:\q\p\zFactory\Llama-tech\_datasets' .

Project directory with experimental configuration files, etc, available at:  'C:\q\p\zFactory\Llama-tech'  or  'C:\core\factory\Llama-tech'  .


JSONL prompt/response fine tuning dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-promptResponse.jsonl  .
```JSONL
{"messages":[{"role":"system","content":""},{"role":"user","content":"PROMPT"},{"role":"assistant","content":"RESPONSE"}]}
```
An autogenerated prompt along the lines of 'please write code matching this description' is used as the prompt, with the segments of code used as the desired response.

JSONL continue the shellcode segment (pairs of example code) fine tuning dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-continuePromptResponse.jsonl  .
```JSONL
{"messages":[{"role":"system","content":""},{"role":"user","content":"CONTINUE_PROMPT"},{"role":"assistant","content":"CONTINUE_RESPONSE"}]}
```
A boilerplate prompt along the lines of 'continue the shellcode'' is used as the prompt, with the segments of code properly enclosed by triple tilde, and autogenerated header/footer added to the response to preserve helpfulness training (which mere continued pre-training presumably would not).

JSONL continue text pre-training dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-continueText.jsonl  .
```JSONL
{"text":"SEGMENT1"}
{"text":"SEGMENT2"}
```
Only the '#===== Segment ####: ubiquitous_bash: lines ###### to ###### ====='  comment is added to these text segments from the original code.


Some axolotl source code may be relevant.

Excerpt: axolotl/src/axolotl/utils/chat_templates.py
```python
_CHAT_TEMPLATES = {
    #...
    "alpaca": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '### Instruction: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ '### Response: ' + message['content'] + eos_token}}{% endif %}{% endfor %}",
    "llama3": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    #...
}
```

Excerpt: axolotl/src/axolotl/prompt_tokenizers.py
```python
LLAMA_DEFAULT_PAD_TOKEN = "<pad>"  # nosec
LLAMA_DEFAULT_EOS_TOKEN = "</s>"  # nosec
LLAMA_DEFAULT_BOS_TOKEN = "<s>"  # nosec
LLAMA_DEFAULT_UNK_TOKEN = "<unk>"  # nosec
```


Some axolotl config documentation and examples may be relevant.

Excerpt: axolotl/docs/config.qmd .
```yaml
# Optional tokenizer configuration path in case you want to use a different tokenizer
# than the one defined in the base model
tokenizer_config:
# If you want to specify the type of model to load, AutoModelForCausalLM is a good choice too
model_type: AutoModelForCausalLM
# Corresponding tokenizer for the model AutoTokenizer is a good choice
tokenizer_type: AutoTokenizer

#...

# This will attempt to quantize the model down to 8 bits and use adam 8 bit optimizer
load_in_8bit: true
# Use bitsandbytes 4 bit
load_in_4bit:

#...

# Do the LoRA/PEFT loading on CPU -- this is required if the base model is so large it takes up most or all of the available GPU VRAM, e.g. during a model and LoRA merge
lora_on_cpu: true

#...

# A list of one or more datasets to finetune the model with
datasets:
  # HuggingFace dataset repo | s3://,gs:// path | "json" for local dataset, make sure to fill data_files
  - path: vicgalle/alpaca-gpt4
    # The type of prompt to use for training. [alpaca, gpteacher, oasst, reflection]
    type: alpaca # format | format:<prompt_style> (chat/instruct) | <prompt_strategies>.load_<load_fn>
    ds_type: # Optional[str] (json|arrow|parquet|text|csv) defines the datatype when path is a file
    data_files: # Optional[str] path to source data files

    shards: # Optional[int] split dataset into N pieces (use with shards_idx)
    shards_idx: # Optional[int] = 0 the index of sharded dataset to use

    preprocess_shards: # Optional[int] process dataset in N sequential chunks for memory efficiency (exclusive with `shards`)

    name: # Optional[str] name of dataset configuration to load
    train_on_split: train # Optional[str] name of dataset split to load from
    revision: # Optional[str] The specific revision of the dataset to use when loading from the Hugging Face Hub. This can be a commit hash, tag, or branch name. If not specified, the latest version will be used. This parameter is ignored for local datasets.
    trust_remote_code: # Optional[bool] Trust remote code for untrusted source

  # Using chat template
  - path: ...
    # Set type to `chat_template` to use this strategy
    type: chat_template
    # Specify the name of the chat template to use
    # The name of the chat template to use for training, following values are supported:
    # - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error. This is the default.
    # - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py
    # - tokenizer_default_fallback_*: where * is the name of the chat template to fallback to if the tokenizer does not have a chat template else default to tokenizer. E.g. tokenizer_default_fallback_chatml.
    # - jinja: Uses a custom jinja template for the chat template. The custom jinja template should be provided in the chat_template_jinja field.
    chat_template: tokenizer_default

    # Custom jinja chat template. Used only if `chat_template: jinja` or empty.
    chat_template_jinja:

    # Key containing the messages (default: "messages")
    field_messages: messages

    # Mapping of properties from the input dataset to the chat template.
    # (default: message_property_mappings={'role':'role', 'content':'content'})
    # If a property exists in the template but not in this mapping, the system will attempt
    # to load it directly from the message using the property name as the key.
    # Example: In the mapping below, 'from' is loaded from input dataset and used as 'role',
    # while 'value' is loaded and used as 'content' in the chat template.
    message_property_mappings:
      role: from
      content: value
      # ...

    # Optional[Dict[str, List]]. Roles mapping in the messages. The default is:
    roles:
      user: ["human", "user"]
      assistant: ["gpt", "assistant"]
      system: ["system"]
      tool: ["tool"]

    # Optional[bool]. Whether to drop the system turn from the dataset. Only works with chat_template.
    # This does not drop the default system message from chat_template if it exists. If you wish to,
    # we recommend using a custom jinja template with the default system message removed or
    # adding a system turn with empty content.
    drop_system_message:

    # IMPORTANT: The following fields determine which parts of the conversation to train on.
    # Priority order: message_field_training > message_field_training_detail > train_on_inputs or role in roles_to_train
    # See examples at `docs/dataset-formats/conversation.qmd`
    # Note: If the below 4 fields are set to empty, defaults to training only on the last message.

    # Optional[List[str]]. Roles to train on. The tokens from these roles will be considered for the loss.
    roles_to_train: ["assistant"]  # default
    # Optional[str]. Which EOS tokens to train on in the conversation. Possible values are:
    # - all: train on all EOS tokens
    # - turn (default): train on the EOS token at the end of each trainable turn
    # - last: train on the last EOS token in the conversation
    # TIP: Please make sure that your `tokenizer.eos_token` is same as EOS/EOT token in template. Otherwise, set `eos_token` under `special_tokens`.
    train_on_eos: last
    # The key in the message turn that indicates via boolean whether tokens of a turn should be considered for training. Useful to selectively train on certain turns besides the `roles_to_train`.
    message_field_training: training
    # The key in the message turn that contains the training details. Useful to selectively train on certain tokens in a turn.
    # The value of the key is a List[Dict] containing `begin_offset` (start character index in content), `end_offset` (end character index in content), and `train` (boolean whether to train).
    message_field_training_detail: train_detail

#...

# The name of the chat template to use for training, following values are supported:
# - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error. This is the default value.
# - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py
# - tokenizer_default_fallback_*: where * is the name of the chat template to fallback to. E.g. tokenizer_default_fallback_chatml. This is useful when the chat template is not available in the tokenizer.
# - jinja: Uses a custom jinja template for the chat template. The custom jinja template should be provided in the chat_template_jinja field.
# The selected chat template will be saved to the tokenizer_config.json for easier inferencing
# Note: It is recommended to set train_on_inputs to true when using a chat template that is different from the model's default chat template.
chat_template: tokenizer_default

#...

# How much of the dataset to set aside as evaluation. 1 = 100%, 0.50 = 50%, etc. 0 for no eval.
val_set_size: 0.04

#...

# The maximum length of an input to train with, this should typically be less than 2048
# as most models have a token/context limit of 2048
sequence_len: 2048

sample_packing:
# Set to 'false' if getting errors during eval with sample_packing on.

#...

# If you want to use 'lora' or 'qlora' or leave blank to train all parameters in original model
adapter: lora

#...

# LoRA hyperparameters
# For more details about the following options, see:
# https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
#  - k_proj
#  - o_proj
#  - gate_proj
#  - down_proj
#  - up_proj
lora_target_linear: # If true, will target all linear modules

#...

# Where to save the full-finetuned model to
output_dir: ./completed-model

#...

# Training hyperparameters

# If greater than 1, backpropagation will be skipped and the gradients will be accumulated for the given number of steps.
gradient_accumulation_steps: 1
# The number of samples to include in each batch. This is the number of samples sent to each GPU.
# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
micro_batch_size: 2
eval_batch_size:
num_epochs: 4
warmup_steps: 100  # cannot use with warmup_ratio
warmup_ratio: 0.05  # cannot use with warmup_steps
learning_rate: 0.00003
lr_quadratic_warmup:
logging_steps:
eval_steps: # Leave empty to eval at each epoch, integer for every N steps. float for fraction of total steps
evals_per_epoch: # number of times per epoch to run evals, mutually exclusive with eval_steps
eval_strategy: # Set to `"no"` to skip evaluation, `"epoch"` at end of each epoch, leave empty to infer from `eval_steps`.
save_strategy: # Set to `"no"` to skip checkpoint saves, `"epoch"` at end of each epoch, `"best"` when better result is achieved, leave empty to infer from `save_steps`.
save_steps: # Leave empty to save at each epoch, integer for every N steps. float for fraction of total steps
saves_per_epoch: # number of times per epoch to save a checkpoint, mutually exclusive with save_steps
save_total_limit: # Checkpoints saved at a time
# Maximum number of iterations to train for. It precedes num_epochs which means that
# if both are set, num_epochs will not be guaranteed.
# e.g., when 1 epoch is 1000 steps => `num_epochs: 2` and `max_steps: 100` will train for 100 steps
max_steps:

#...

# whether to find batch size that fits in memory. Passed to underlying transformers Trainer
auto_find_batch_size: # Optional[bool]

#...

loss_watchdog_threshold: # High loss value, indicating the learning has broken down (a good estimate is ~2 times the loss at the start of training)
loss_watchdog_patience: # Number of high-loss steps in a row before the trainer aborts (default: 3)

#...

# Optional[bool]. Whether to use flash attention patch https://github.com/Dao-AILab/flash-attention:
flash_attention:

#...

# FSDP
fsdp:
fsdp_config:

# Deepspeed config path. e.g., deepspeed_configs/zero3.json
deepspeed:

#...

# Seed
seed:

# Allow overwrite yml config using from cli
strict:
```

Excerpt: axolotl/examples/llama-3/instruct-lora-8b.yml
```yaml
base_model: NousResearch/Meta-Llama-3-8B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

#...

chat_template: llama3
datasets:
  - path: fozziethebeat/alpaca_messages_2k_test
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      user:
        - user
      assistant:
        - assistant

#...

val_set_size: 0.05
output_dir: ./outputs/lora-out

#...

sequence_len: 4096
sample_packing: false
pad_to_sequence_len: true

adapter: lora
lora_model_dir:
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

#...

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 4
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
evals_per_epoch: 4
eval_table_size:
eval_max_new_tokens: 128
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
   pad_token: <|end_of_text|>
```

Excerpt: axolotl/examples/llama-3/lora-8b.yml
```yaml
base_model: NousResearch/Meta-Llama-3-8B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

#...

lora_modules_to_save:
  - embed_tokens
  - lm_head
```

Excerpt: axolotl/examples/llama-3/lora-1b.yml
```yaml
base_model: NousResearch/Llama-3.2-1B

#...

sequence_len: 2048

#...

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_fan_in_fan_out:
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj

#...

gradient_accumulation_steps: 2
micro_batch_size: 2
num_epochs: 1
optimizer: adamw_8bit
lr_scheduler: cosine
learning_rate: 0.0002

#...

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3
```

Excerpt: axolotl/examples/llama-3/qlora-fsdp-405b.yaml
```yaml
base_model: hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16
tokenizer_type: AutoTokenizer

#...

load_in_4bit: true

#...

val_set_size: 0.0

#...

sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
lora_target_linear: true

gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 2
optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 0.00001

#...

warmup_steps: 10
evals_per_epoch: 4
saves_per_epoch: 1
weight_decay: 0.0
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_limit_all_gathers: true
  fsdp_sync_module_states: true
  fsdp_offload_params: true
  fsdp_use_orig_params: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD
special_tokens:
  pad_token: <|finetune_right_pad_id|>
```


Reported LORA parameter recommendations are contradictory yet possibly enlightening. 
```
from https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2

Hyperparameters

Rank: 8
tests suggested minimal performance boosts when increasing the rank to, for instance, 16 ... settled on a rank of 8 to maintain smaller checkpoint sizes and to avoid artificially inflating our checkpoint files

Alpha: 16
Alpha scales the learned weights. Existing literature, including the original LoRA paper, generally advises fixing Alpha—often at 16—rather than treating it as a tunable hyperparameter.

Target modules: All dense layers
subsequent work has shown that targeting additional layers, or even all layers, can improve performance ... hypothesize that applying LoRA to a greater number of layers brings us closer to achieving the capabilities of full-parameter fine-tuning

Base learning rate: 1e-4
A learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. Although we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing the process...
```
```
from https://www.theregister.com/2024/11/10/llm_finetuning_guide/?page=6

hyperparameters

lora_r: Defines how large the LoRA matrices used to train the model are and by extension, how many weights are ultimately updated. The larger the rank, the more weights that get fine-tuned.

lora_alpha: Sets a scaling factor applied to weight changes when they're added to the original weights. In our research, we found that practice appears to be to set lora_alpha to around a fourth lora_rank. So if lora_r is set to 64, lora_alpha should be set to 16.

lora_dropout: Helps to avoid a phenomenon called overfitting by randomly setting some weight changes to zero. In the original QLoRA paper, researchers found that a dropout rate of 0.05 was effective for smaller models in the 7 to 13 billion parameter range.
```


Possibly DUBIOUS configuration options:
```yaml
# from https://axolotl-ai-cloud.github.io/axolotl/docs/inference.html
special_tokens:
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
tokens:
  - "<|im_start|>"
  - "<|im_end|>"
```


Command 'accelerate' is known to launch a python script.
```
accelerate launch --help
#...
Launch a python script in a distributed scenario.
```

Apparently, 'accelerate' command launches scripts from a '/scripts' subdirectory, though these scripts are apparently not present in the axolotl repository scripts subdirectory.
from https://www.animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/
```bash
accelerate config  # selected no distributed training and defaults

# This will take some time. Output will be in `./qlora-out`
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml

# When training finishes, you can test inference with this:
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml --inference --lora_model_dir="./qlora-out"

# Merge the lora weights into one file
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml --merge_lora --lora_model_dir="./qlora-out" --load_in_8bit=False --load_in_4bit=False
```


Known possibly relevant, possibly DUBIOUS, commands:
```bash
axolotl preprocess /workspace/project/experiment-ubiquitous_bash-lora.yml

#--deepspeed deepspeed_configs/zero1.json
axolotl train /workspace/project/experiment-ubiquitous_bash-lora.yml

axolotl inference /workspace/project/experiment-ubiquitous_bash-lora.yml


# DUBIOUS
#CUDA_VISIBLE_DEVICES="" python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml
#accelerate launch --num_processes=2 -m /workspace/project/Llama-augment-ubiquitous_bash-lora.yml --deepspeed deepspeed_configs/zero1.json
#axolotl train /workspace/project/Llama-augment-ubiquitous_bash-lora.yml



# DUBIOUS - from https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl
#pip3 install packaging ninja
#pip3 install -e '.[flash-attn,deepspeed]'

#CUDA_VISIBLE_DEVICES=""
#python -m axolotl.cli.preprocess examples/openllama-3b/lora.yml

#accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml

## Very DUBIOUS . Seems this does not accept simple text prompting, requiring at least prompt formatting, and maybe only responding correctly to specialized JSON beyond that.
## https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/
#accelerate launch -m axolotl.cli.inference examples/openllama-3b/lora.yml --lora_model_dir="./lora-out"

#python3 -m axolotl.cli.merge_lora examples/openllama-3b/lora.yml --lora_model_dir="./lora-out"


# DUBIOUS - from https://axolotl-ai-cloud.github.io/axolotl/docs/inference.html
## Very DUBIOUS . Seems this does not accept simple text prompting, requiring at least prompt formatting, and maybe only responding correctly to specialized JSON beyond that.
axolotl inference your_config.yml --lora-model-dir="./lora-output-dir"
axolotl inference your_config.yml --base-model="./completed-model"
axolotl inference your_config.yml --gradio
cat /tmp/prompt.txt | axolotl inference your_config.yml --base-model="./completed-model" --prompter=None
axolotl inference your_config.yml --load-in-8bit=True


# DUBIOUS - from https://axolotl-ai-cloud.github.io/axolotl/docs/inference.html
## Configuration Options (alternative)
#  #gpu_memory_limit: 20GiB  # Adjust based on your GPU
#  #lora_on_cpu: true        # Process on CPU if needed
#CUDA_VISIBLE_DEVICES="" axolotl merge-lora ...

# from https://axolotl-ai-cloud.github.io/axolotl/docs/inference.html
#axolotl preprocess your_config.yml --debug
```


Quantize, Inference, commands from https://medium.com/@qdrddr/the-easiest-way-to-convert-a-model-to-gguf-and-quantize-91016e97c987 .
```bash
# Docker

mkdir -p ~/models
huggingface-cli login
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.3 --local-dir "~/models" --include "*"

#Convert to GGUF
docker run --rm -v "~/models":/repo ghcr.io/ggerganov/llama.cpp:full --convert "/repo" --outtype f32
ls ~/models | grep .gguf
#> ggml-model-f32.gguf

#Quantize from F32.gguf to Q4_K_M.bin
docker run --rm -v "~/models":/repo ghcr.io/ggerganov/llama.cpp:full --quantize "/repo/ggml-model-f32.gguf" "/repo/ggml-model-Q4_K_M.bin" "Q4_K_M"
ls ~/models | grep .bin
#> ggml-model-Q4_K_M.bin


# ollama (alternative)

#GGUF to q6_K
echo "FROM hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:F16" > "~/models/modelfile"
ollama create hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:q6_K --quantize q6_K --file ~/models/modelfile


#Safetensors
model=sentence-transformers-testing/stsb-bert-tiny-safetensors
modelname=hf.co/${model}
modeldir=${PWD}/${model}
mkdir -p "${modeldir}" && huggingface-cli download "${model}" --local-dir "${modeldir}" --include "*"
echo "FROM ${modeldir}" > "${modeldir}/modelfile"
ollama create $modelname -f ${modeldir}/modelfile
ollama list | grep $modelname


# ollama without temporary GGUF

model=mistralai/Mistral-7B-Instruct-v0.3
modelname=Mistral:7b-Instruct-v0.3
modelfolder=${PWD}/${model}

huggingface-cli login
huggingface-cli download "${model}" --local-dir "${modelfolder}" --include "*"

#Pointing to directory with PyTorch/Safetensors might not always work, try creating GGUF 
echo "FROM ${modeldir}" > "${modeldir}/modelfile"
ollama create "${modelname}" -f "${modeldir}/modelfile"

#If pointing to folder doesn't work, then with GGUF it should work:
echo "FROM ${modeldir}/ggml-model-f16.gguf" > "${modeldir}/modelfile"
ollama create "${modelname}" -f "${modeldir}/modelfile"
```


Quantize, Inference, commands from https://medium.com/@kevin.lopez.91/simple-tutorial-to-quantize-models-using-llama-cpp-from-safetesnsors-to-gguf-c42acf2c537d .

```bash
sudo apt-get install git git-lfs
git lfs install
git-lfs clone https://huggingface.co/microsoft/phi-2

conda create — name phi_llm
conda activate phi_llm

git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp/
pip install -r requirements.txt
make -j8 
# if you have cuda enabled Try using the below instead
# make LLAMA_CUBLAS=1 -j8

# use the convert script to convert the models from hf to gguf
./convert-hf-to-gguf.py ../phi-2/ — outfile phi-2_fp16.gguf
cd ../

# convert the model from fp16 to q4
./llama.cpp/quantize phi-2_fp16.gguf phi-2_Q4_K_M.gguf Q4_K_M

# test
llama.cpp/main — model phi-2_Q4_K_M.gguf — interactive
# if you want to use a GPU then try: 
# llama.cpp/main — model phi-2_Q4_K_M.gguf — interactive -ngl <number of layers your gpu can handle (3090 can do all layers)>


# alternative

pip install llama-cpp-python
# If you want to use cuda try this:
# CMAKE_ARGS=”-DLLAMA_CUBLAS=on” pip install llama-cpp-python

python3
```
```python
import os
import argparse
import llama_cpp
from llama_cpp import llama_model_quantize_params

result = llama_cpp.llama_model_quantize("phi-2_fp16.gguf".encode("utf-8"), "phi-2_Q4_1_low_level.gguf".encode("utf-8"), llama_model_quantize_params(0,3,True, True, False))
```
```bash
# test
llama.cpp/main — model phi-2_Q4_K_M.gguf — interactive
# if you want to use a GPU then try: 
# llama.cpp/main — model phi-2_Q4_K_M.gguf — interactive -ngl <number of layers your gpu can handle (3090 can do all layers)>
```


Quantize, Inference, GGML commands from https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172/ .
```bash
# Install llama.cpp
!git clone https://github.com/ggerganov/llama.cpp
!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make
!pip install -r llama.cpp/requirements.txt

MODEL_ID = "mlabonne/EvolCodeLlama-7b"

# Download model
!git lfs install
!git clone https://huggingface.co/{MODEL_ID}

MODEL_NAME = MODEL_ID.split('/')[-1]

# Convert to fp16
fp16 = f"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin"
!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}

QUANTIZATION_METHODS = ["q4_k_m", "q5_k_m"]

for method in QUANTIZATION_METHODS:
    qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf"
    !./llama.cpp/quantize {fp16} {qtype} {method}

# Our two quantized models are now ready for inference.
```
```python
import os

model_list = [file for file in os.listdir(MODEL_NAME) if "gguf" in file]

prompt = input("Enter your prompt: ")
chosen_method = input("Name of the model (options: " + ", ".join(model_list) + "): ")

# Verify the chosen method is in the list
if chosen_method not in model_list:
    print("Invalid name")
else:
    qtype = f"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf"
    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p "{prompt}"
```


Fine Tune, Quantize, Inference, commands from https://www.animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/ .
```
# Setup a new conda environment pinned to python 3.9
conda create -n axolotl python=3.9
conda activate axolotl

# Install pytorch for cuda 11.8
pip3 install torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu118

# Clone the github and switch directories to it
git clone https://github.com/OpenAccess-AI-Collective/axolotl
cd axolotl

# As of the time of this writing, 0.2.1 is the latest release
git checkout tags/v0.2.1

# Install the dependencies
pip3 install -e .
pip3 install -U git+https://github.com/huggingface/peft.git

# I have problems with the current bitandbytes unless I force
# the the cuda 11.8 version onto the cpu version ...
cd ~/miniconda3/envs/axolotl/lib/python3.9/site-packages/bitsandbytes
mv libbitsandbytes_cpu.so backup_libbitsandbytes_cpu.so
cp libbitsandbytes_cuda118.so libbitsandbytes_cpu.so

cd ~/axolotl
accelerate config  # selected no distributed training and defaults

# Copy the 3B qlora example for open-llama into a new directory
mkdir examples/openllama-7b
cp examples/openllama-3b/qlora.yml \
    examples/openllama-7b/qlora.yml

vim examples/openllama-7b/qlora.yml
## EDIT this qlora.yml to change these keys to target the 7B model
#    base_model: openlm-research/open_llama_7b
#    base_model_config: openlm-research/open_llama_7b

# This will take some time. Output will be in `./qlora-out`
accelerate launch scripts/finetune.py \
    examples/openllama-7b/qlora.yml

# When training finishes, you can test inference with this:
accelerate launch scripts/finetune.py \
    examples/openllama-7b/qlora.yml \
    --inference --lora_model_dir="./qlora-out"

# Merge the lora weights into one file
accelerate launch scripts/finetune.py \
    examples/openllama-7b/qlora.yml \
    --merge_lora --lora_model_dir="./qlora-out" \
    --load_in_8bit=False --load_in_4bit=False

# Now we have a merged model in ./qlora-out/merged
# We need to copy the tokenizer.model back into this directory
cd qlora-out/merged
wget https://huggingface.co/openlm-research/open_llama_7b/resolve/main/tokenizer.model

# Setup llama.cpp for quantization and inference 
# (steps shown for linux; ymmv)
cd $HOME
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make LLAMA_CUBLAS=1

# We need to convert the pytorch model into ggml for quantization
# It crates 'ggml-model-f16.bin' in the 'merged' directory.
python convert.py --outtype f16 \
    ~/axolotl/qlora-out/merged/pytorch_model-00001-of-00002.bin 

# Start off by making a basic q4_0 4-bit quantization.
# It's important to have 'ggml' in the name of the quant for some 
# software to recognize it's file format. 
./quantize ~/axolotl/qlora-out/merged/ggml-model-f16.bin \
    ~/axolotl/qlora-out/merged/openllama-7b-GPT4-ggml-q4_0.bin q4_0

# There we go! Now we have a quantized fine-tuned model! 
# You can test it out with llama.cpp
./main -n 128 --color -i -r "User:" -f prompts/chat-with-bob.txt \
    -m ~/axolotl/qlora-out/merged/openllama-7b-GPT4-ggml-q4_0.bin
```


Inference, Quantize commands and Modelfile parameters from https://github.com/ollama/ollama/blob/main/docs/import.md .

Importing a fine tuned adapter from Safetensors weights
```Modelfile
FROM <base model name>
ADAPTER /path/to/safetensors/adapter/directory
```
```bash
ollama create my-model
ollama create my-model
```

Importing a model from Safetensors weights
```Modelfile
FROM /path/to/safetensors/directory
```
```bash
ollama create my-model
```
```bash
ollama run my-model
```

Importing a GGUF based model or adapter
```Modelfile
#To import a GGUF model, create a Modelfile containing:
FROM /path/to/file.gguf
```
```Modelfile
#(alternative)
#For a GGUF adapter, create the Modelfile with:
FROM <model name>
ADAPTER /path/to/file.gguf
```
```bash
ollama create my-model
```

Quantizing a Model
```Modelfile
FROM /path/to/my/gemma/f16/model
```
```bash
ollama create --quantize q4_K_M mymodel
```




Please suggest commands, configuration, etc, to fine tune , quantize , and inference/test , Llama 3.1 8b , etc, using such compatible techiniques as axolotl , ollama , etc. Later fine-tuning is planned for Llama 3.1 405b using more powerful hardware from a runpod instance. Purely for low-VRAM experiments Llama 3.2 1b may be fine tuned as well. Dataset usability with such model fine-tuning services as OpenAI is also preferred.





