
Axolotl Docker container installed on a Cygwin/MSW platform .
```bash
#--privileged
#--ipc=host --ulimit memlock=-1 --ulimit stack=67108864
docker run --shm-size=20g --name axolotl-$(_uid 14) --gpus "all" -v 'C:\q':/q -v 'C:\core':/core -v "$USERPROFILE"'\Downloads':/Downloads -v "$factory_outputsDir":/outputs -v "$factory_modelDir":/model -v "$factory_datasetsDir":/datasets -v "$factory_projectDir":/workspace/project --rm -it axolotlai/axolotl:main-latest
```


Models (eg. Meta-Llama-3.1-8B-Instruct-abliterated , Llama-3.2-1B ) in safetensors format available at: 'C:\q\p\zFactory\Llama-tech\models' or similar .
Outputs (eg. LORA adapters) saved at: 'C:\q\p\zFactory\Llama-tech\outputs' or similar .

Datasets (eg. JSONL prompt/response fine tuning) available at: 'C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset' or 'C:\q\p\zFactory\Llama-tech\_datasets' .

Project directory with experimental configuration files, etc, available at:  'C:\q\p\zFactory\Llama-tech'  or  'C:\core\factory\Llama-tech'  .


JSONL prompt/response fine tuning dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-promptResponse.jsonl  .
```JSONL
{"messages":[{"role":"system","content":""},{"role":"user","content":"PROMPT"},{"role":"assistant","content":"RESPONSE"}]}
```
An autogenerated prompt along the lines of 'please write code matching this description' is used as the prompt, with the segments of code used as the desired response.

JSONL continue the shellcode segment (pairs of example code) fine tuning dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-continuePromptResponse.jsonl  .
```JSONL
{"messages":[{"role":"system","content":""},{"role":"user","content":"CONTINUE_PROMPT"},{"role":"assistant","content":"CONTINUE_RESPONSE"}]}
```
A boilerplate prompt along the lines of 'continue the shellcode'' is used as the prompt, with the segments of code properly enclosed by triple tilde, and autogenerated header/footer added to the response to preserve helpfulness training (which mere continued pre-training presumably would not).

JSONL continue text pre-training dataset at:  C:\q\p\zCore\infrastructure\ubiquitous_bash\_local\dataset\ubiquitous_bash_finetuning-continueText.jsonl  .
```JSONL
{"text":"SEGMENT1"}
{"text":"SEGMENT2"}
```
Only the '#===== Segment ####: ubiquitous_bash: lines ###### to ###### ====='  comment is added to these text segments from the original code.


Some axolotl source code may be relevant.

Excerpt: axolotl/src/axolotl/utils/chat_templates.py
```python
_CHAT_TEMPLATES = {
    #...
    "alpaca": "{% for message in messages %}{% if message['role'] == 'user' %}{{ '### Instruction: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ '### Response: ' + message['content'] + eos_token}}{% endif %}{% endfor %}",
    "llama3": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
    #...
}
```

Excerpt: axolotl/src/axolotl/prompt_tokenizers.py
```python
LLAMA_DEFAULT_PAD_TOKEN = "<pad>"  # nosec
LLAMA_DEFAULT_EOS_TOKEN = "</s>"  # nosec
LLAMA_DEFAULT_BOS_TOKEN = "<s>"  # nosec
LLAMA_DEFAULT_UNK_TOKEN = "<unk>"  # nosec
```


Some axolotl config documentation and examples may be relevant.

Excerpt: axolotl/docs/config.qmd .
```yaml
# Optional tokenizer configuration path in case you want to use a different tokenizer
# than the one defined in the base model
tokenizer_config:
# If you want to specify the type of model to load, AutoModelForCausalLM is a good choice too
model_type: AutoModelForCausalLM
# Corresponding tokenizer for the model AutoTokenizer is a good choice
tokenizer_type: AutoTokenizer

#...

# This will attempt to quantize the model down to 8 bits and use adam 8 bit optimizer
load_in_8bit: true
# Use bitsandbytes 4 bit
load_in_4bit:

#...

# Do the LoRA/PEFT loading on CPU -- this is required if the base model is so large it takes up most or all of the available GPU VRAM, e.g. during a model and LoRA merge
lora_on_cpu: true

#...

# A list of one or more datasets to finetune the model with
datasets:
  # HuggingFace dataset repo | s3://,gs:// path | "json" for local dataset, make sure to fill data_files
  - path: vicgalle/alpaca-gpt4
    # The type of prompt to use for training. [alpaca, gpteacher, oasst, reflection]
    type: alpaca # format | format:<prompt_style> (chat/instruct) | <prompt_strategies>.load_<load_fn>
    ds_type: # Optional[str] (json|arrow|parquet|text|csv) defines the datatype when path is a file
    data_files: # Optional[str] path to source data files

    shards: # Optional[int] split dataset into N pieces (use with shards_idx)
    shards_idx: # Optional[int] = 0 the index of sharded dataset to use

    preprocess_shards: # Optional[int] process dataset in N sequential chunks for memory efficiency (exclusive with `shards`)

    name: # Optional[str] name of dataset configuration to load
    train_on_split: train # Optional[str] name of dataset split to load from
    revision: # Optional[str] The specific revision of the dataset to use when loading from the Hugging Face Hub. This can be a commit hash, tag, or branch name. If not specified, the latest version will be used. This parameter is ignored for local datasets.
    trust_remote_code: # Optional[bool] Trust remote code for untrusted source

  # Using chat template
  - path: ...
    # Set type to `chat_template` to use this strategy
    type: chat_template
    # Specify the name of the chat template to use
    # The name of the chat template to use for training, following values are supported:
    # - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error. This is the default.
    # - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py
    # - tokenizer_default_fallback_*: where * is the name of the chat template to fallback to if the tokenizer does not have a chat template else default to tokenizer. E.g. tokenizer_default_fallback_chatml.
    # - jinja: Uses a custom jinja template for the chat template. The custom jinja template should be provided in the chat_template_jinja field.
    chat_template: tokenizer_default

    # Custom jinja chat template. Used only if `chat_template: jinja` or empty.
    chat_template_jinja:

    # Key containing the messages (default: "messages")
    field_messages: messages

    # Mapping of properties from the input dataset to the chat template.
    # (default: message_property_mappings={'role':'role', 'content':'content'})
    # If a property exists in the template but not in this mapping, the system will attempt
    # to load it directly from the message using the property name as the key.
    # Example: In the mapping below, 'from' is loaded from input dataset and used as 'role',
    # while 'value' is loaded and used as 'content' in the chat template.
    message_property_mappings:
      role: from
      content: value
      # ...

    # Optional[Dict[str, List]]. Roles mapping in the messages. The default is:
    roles:
      user: ["human", "user"]
      assistant: ["gpt", "assistant"]
      system: ["system"]
      tool: ["tool"]

    # Optional[bool]. Whether to drop the system turn from the dataset. Only works with chat_template.
    # This does not drop the default system message from chat_template if it exists. If you wish to,
    # we recommend using a custom jinja template with the default system message removed or
    # adding a system turn with empty content.
    drop_system_message:

    # IMPORTANT: The following fields determine which parts of the conversation to train on.
    # Priority order: message_field_training > message_field_training_detail > train_on_inputs or role in roles_to_train
    # See examples at `docs/dataset-formats/conversation.qmd`
    # Note: If the below 4 fields are set to empty, defaults to training only on the last message.

    # Optional[List[str]]. Roles to train on. The tokens from these roles will be considered for the loss.
    roles_to_train: ["assistant"]  # default
    # Optional[str]. Which EOS tokens to train on in the conversation. Possible values are:
    # - all: train on all EOS tokens
    # - turn (default): train on the EOS token at the end of each trainable turn
    # - last: train on the last EOS token in the conversation
    # TIP: Please make sure that your `tokenizer.eos_token` is same as EOS/EOT token in template. Otherwise, set `eos_token` under `special_tokens`.
    train_on_eos: last
    # The key in the message turn that indicates via boolean whether tokens of a turn should be considered for training. Useful to selectively train on certain turns besides the `roles_to_train`.
    message_field_training: training
    # The key in the message turn that contains the training details. Useful to selectively train on certain tokens in a turn.
    # The value of the key is a List[Dict] containing `begin_offset` (start character index in content), `end_offset` (end character index in content), and `train` (boolean whether to train).
    message_field_training_detail: train_detail

#...

# The name of the chat template to use for training, following values are supported:
# - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error. This is the default value.
# - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py
# - tokenizer_default_fallback_*: where * is the name of the chat template to fallback to. E.g. tokenizer_default_fallback_chatml. This is useful when the chat template is not available in the tokenizer.
# - jinja: Uses a custom jinja template for the chat template. The custom jinja template should be provided in the chat_template_jinja field.
# The selected chat template will be saved to the tokenizer_config.json for easier inferencing
# Note: It is recommended to set train_on_inputs to true when using a chat template that is different from the model's default chat template.
chat_template: tokenizer_default

#...

# How much of the dataset to set aside as evaluation. 1 = 100%, 0.50 = 50%, etc. 0 for no eval.
val_set_size: 0.04

#...

# The maximum length of an input to train with, this should typically be less than 2048
# as most models have a token/context limit of 2048
sequence_len: 2048

sample_packing:
# Set to 'false' if getting errors during eval with sample_packing on.

#...

# If you want to use 'lora' or 'qlora' or leave blank to train all parameters in original model
adapter: lora

#...

# LoRA hyperparameters
# For more details about the following options, see:
# https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
#  - k_proj
#  - o_proj
#  - gate_proj
#  - down_proj
#  - up_proj
lora_target_linear: # If true, will target all linear modules

#...

# Where to save the full-finetuned model to
output_dir: ./completed-model

#...

# Training hyperparameters

# If greater than 1, backpropagation will be skipped and the gradients will be accumulated for the given number of steps.
gradient_accumulation_steps: 1
# The number of samples to include in each batch. This is the number of samples sent to each GPU.
# Batch size per gpu = micro_batch_size * gradient_accumulation_steps
micro_batch_size: 2
eval_batch_size:
num_epochs: 4
warmup_steps: 100  # cannot use with warmup_ratio
warmup_ratio: 0.05  # cannot use with warmup_steps
learning_rate: 0.00003
lr_quadratic_warmup:
logging_steps:
eval_steps: # Leave empty to eval at each epoch, integer for every N steps. float for fraction of total steps
evals_per_epoch: # number of times per epoch to run evals, mutually exclusive with eval_steps
eval_strategy: # Set to `"no"` to skip evaluation, `"epoch"` at end of each epoch, leave empty to infer from `eval_steps`.
save_strategy: # Set to `"no"` to skip checkpoint saves, `"epoch"` at end of each epoch, `"best"` when better result is achieved, leave empty to infer from `save_steps`.
save_steps: # Leave empty to save at each epoch, integer for every N steps. float for fraction of total steps
saves_per_epoch: # number of times per epoch to save a checkpoint, mutually exclusive with save_steps
save_total_limit: # Checkpoints saved at a time
# Maximum number of iterations to train for. It precedes num_epochs which means that
# if both are set, num_epochs will not be guaranteed.
# e.g., when 1 epoch is 1000 steps => `num_epochs: 2` and `max_steps: 100` will train for 100 steps
max_steps:

#...

# whether to find batch size that fits in memory. Passed to underlying transformers Trainer
auto_find_batch_size: # Optional[bool]

#...

loss_watchdog_threshold: # High loss value, indicating the learning has broken down (a good estimate is ~2 times the loss at the start of training)
loss_watchdog_patience: # Number of high-loss steps in a row before the trainer aborts (default: 3)

#...

# Optional[bool]. Whether to use flash attention patch https://github.com/Dao-AILab/flash-attention:
flash_attention:

#...

# FSDP
fsdp:
fsdp_config:

# Deepspeed config path. e.g., deepspeed_configs/zero3.json
deepspeed:

#...

# Seed
seed:

# Allow overwrite yml config using from cli
strict:
```

Excerpt: axolotl/examples/llama-3/instruct-lora-8b.yml
```yaml
base_model: NousResearch/Meta-Llama-3-8B-Instruct
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

#...

chat_template: llama3
datasets:
  - path: fozziethebeat/alpaca_messages_2k_test
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      user:
        - user
      assistant:
        - assistant

#...

val_set_size: 0.05
output_dir: ./outputs/lora-out

#...

sequence_len: 4096
sample_packing: false
pad_to_sequence_len: true

adapter: lora
lora_model_dir:
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

#...

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 4
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true
s2_attention:

warmup_steps: 10
evals_per_epoch: 4
eval_table_size:
eval_max_new_tokens: 128
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
   pad_token: <|end_of_text|>
```

Excerpt: axolotl/examples/llama-3/lora-8b.yml
```yaml
base_model: NousResearch/Meta-Llama-3-8B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer

#...

lora_modules_to_save:
  - embed_tokens
  - lm_head
```

Excerpt: axolotl/examples/llama-3/lora-1b.yml
```yaml
base_model: NousResearch/Llama-3.2-1B

#...

sequence_len: 2048

#...

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_fan_in_fan_out:
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj

#...

gradient_accumulation_steps: 2
micro_batch_size: 2
num_epochs: 1
optimizer: adamw_8bit
lr_scheduler: cosine
learning_rate: 0.0002

#...

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3
```

Excerpt: axolotl/examples/llama-3/qlora-fsdp-405b.yaml
```yaml
base_model: hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16
tokenizer_type: AutoTokenizer

#...

load_in_4bit: true

#...

val_set_size: 0.0

#...

sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
lora_target_linear: true

gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 2
optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 0.00001

#...

warmup_steps: 10
evals_per_epoch: 4
saves_per_epoch: 1
weight_decay: 0.0
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_limit_all_gathers: true
  fsdp_sync_module_states: true
  fsdp_offload_params: true
  fsdp_use_orig_params: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD
special_tokens:
  pad_token: <|finetune_right_pad_id|>
```


Reported LORA parameter recommendations are contradictory yet possibly enlightening. 
```
from https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2

Hyperparameters

Rank: 8
tests suggested minimal performance boosts when increasing the rank to, for instance, 16 ... settled on a rank of 8 to maintain smaller checkpoint sizes and to avoid artificially inflating our checkpoint files

Alpha: 16
Alpha scales the learned weights. Existing literature, including the original LoRA paper, generally advises fixing Alpha—often at 16—rather than treating it as a tunable hyperparameter.

Target modules: All dense layers
subsequent work has shown that targeting additional layers, or even all layers, can improve performance ... hypothesize that applying LoRA to a greater number of layers brings us closer to achieving the capabilities of full-parameter fine-tuning

Base learning rate: 1e-4
A learning rate of 1e-4 has become the standard when fine-tuning LLMs with LoRA. Although we occasionally encountered training loss instabilities, reducing the learning rate to lower values like 3e-5 proved effective in stabilizing the process...
```
```
from https://www.theregister.com/2024/11/10/llm_finetuning_guide/?page=6

hyperparameters

lora_r: Defines how large the LoRA matrices used to train the model are and by extension, how many weights are ultimately updated. The larger the rank, the more weights that get fine-tuned.

lora_alpha: Sets a scaling factor applied to weight changes when they're added to the original weights. In our research, we found that practice appears to be to set lora_alpha to around a fourth lora_rank. So if lora_r is set to 64, lora_alpha should be set to 16.

lora_dropout: Helps to avoid a phenomenon called overfitting by randomly setting some weight changes to zero. In the original QLoRA paper, researchers found that a dropout rate of 0.05 was effective for smaller models in the 7 to 13 billion parameter range.
```


Possibly DUBIOUS configuration options:
```yaml
# from https://axolotl-ai-cloud.github.io/axolotl/docs/inference.html
special_tokens:
  bos_token: "<s>"
  eos_token: "</s>"
  unk_token: "<unk>"
tokens:
  - "<|im_start|>"
  - "<|im_end|>"
```


Command 'accelerate' is known to launch a python script.
```
accelerate launch --help
#...
Launch a python script in a distributed scenario.
```

Apparently, 'accelerate' command launches scripts from a '/scripts' subdirectory, though these scripts are apparently not present in the axolotl repository scripts subdirectory.
from https://www.animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/
```bash
accelerate config  # selected no distributed training and defaults

# This will take some time. Output will be in `./qlora-out`
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml

# When training finishes, you can test inference with this:
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml --inference --lora_model_dir="./qlora-out"

# Merge the lora weights into one file
accelerate launch scripts/finetune.py examples/openllama-7b/qlora.yml --merge_lora --lora_model_dir="./qlora-out" --load_in_8bit=False --load_in_4bit=False
```


Known commands:
```bash
axolotl preprocess /workspace/project/experiment-ubiquitous_bash-lora.yml

#--deepspeed deepspeed_configs/zero1.json
axolotl train /workspace/project/experiment-ubiquitous_bash-lora.yml

# DUBIOUS
axolotl inference /workspace/project/experiment-ubiquitous_bash-lora.yml



axolotl merge-lora /workspace/project/experiment-ubiquitous_bash-lora.yml --lora-model-dir="/outputs/experiment-ubiquitous_bash/lora-out" --output-dir="/outputs/experiment-ubiquitous_bash"



cd /workspace/project/_lib
##git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
##make clean && make LLAMA_CUBLAS=1
cmake -B build
cmake --build build --config Release
pip install torch transformers safetensors sentencepiece

python convert_hf_to_gguf.py /outputs/experiment-ubiquitous_bash/merged --outfile /outputs/experiment-ubiquitous_bash-lora-f32.gguf --outtype f32



cat <<'EOF' > experiment-ubiquitous_bash.Modelfile
FROM ./outputs/experiment-ubiquitous_bash-lora-f32.gguf
#FROM ./models/Llama-3.2-1B
#ADAPTER ./outputs/experiment-ubiquitous_bash/lora-out

TEMPLATE """<|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023

{{ if .System }}{{ .System }}
{{- end }}
{{- if .Tools }}When you receive a tool call response, use the output to format an answer to the orginal user question.

You are a helpful assistant with tool calling capabilities.
{{- end }}<|eot_id|>
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 }}
{{- if eq .Role "user" }}<|start_header_id|>user<|end_header_id|>
{{- if and $.Tools $last }}

Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.

Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}. Do not use variables.

{{ range $.Tools }}
{{- . }}
{{ end }}
{{ .Content }}<|eot_id|>
{{- else }}

{{ .Content }}<|eot_id|>
{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>

{{ end }}
{{- else if eq .Role "assistant" }}<|start_header_id|>assistant<|end_header_id|>
{{- if .ToolCalls }}
{{ range .ToolCalls }}
{"name": "{{ .Function.Name }}", "parameters": {{ .Function.Arguments }}}{{ end }}
{{- else }}

{{ .Content }}
{{- end }}{{ if not $last }}<|eot_id|>{{ end }}
{{- else if eq .Role "tool" }}<|start_header_id|>ipython<|end_header_id|>

{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>

{{ end }}
{{- end }}
{{- end }}"""

LICENSE "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT"

EOF

ollama create llama3-ubiquitous-bash-1b -f experiment-ubiquitous_bash.Modelfile
```



Docker containers reportedly exist for llama.cpp . From https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md .
```containers
We have three Docker images available for this project:

ghcr.io/ggml-org/llama.cpp:full: This image includes both the main executable file and the tools to convert LLaMA models into ggml and convert into 4-bit quantization. (platforms: linux/amd64, linux/arm64)
ghcr.io/ggml-org/llama.cpp:light: This image only includes the main executable file. (platforms: linux/amd64, linux/arm64)
ghcr.io/ggml-org/llama.cpp:server: This image only includes the server executable file. (platforms: linux/amd64, linux/arm64)
Additionally, there the following images, similar to the above:

ghcr.io/ggml-org/llama.cpp:full-cuda: Same as full but compiled with CUDA support. (platforms: linux/amd64)
ghcr.io/ggml-org/llama.cpp:light-cuda: Same as light but compiled with CUDA support. (platforms: linux/amd64)
ghcr.io/ggml-org/llama.cpp:server-cuda: Same as server but compiled with CUDA support. (platforms: linux/amd64)
ghcr.io/ggml-org/llama.cpp:full-rocm: Same as full but compiled with ROCm support. (platforms: linux/amd64, linux/arm64)
ghcr.io/ggml-org/llama.cpp:light-rocm: Same as light but compiled with ROCm support. (platforms: linux/amd64, linux/arm64)
ghcr.io/ggml-org/llama.cpp:server-rocm: Same as server but compiled with ROCm support. (platforms: linux/amd64, linux/arm64)
ghcr.io/ggml-org/llama.cpp:full-musa: Same as full but compiled with MUSA support. (platforms: linux/amd64)
ghcr.io/ggml-org/llama.cpp:light-musa: Same as light but compiled with MUSA support. (platforms: linux/amd64)
ghcr.io/ggml-org/llama.cpp:server-musa: Same as server but compiled with MUSA support. (platforms: linux/amd64)
```


Newer versions of llama.cpp apparently do not have './quantize' .



Autogenerated ubiquitous_bash prompt/response dataset entry for the ubiquitous_bash dataset .
```dataset
{"messages":[{"role":"system","content":""},{"role":"user","content":"\nPlease invent a self-contained fragment of exemplary well crafted very creative bash shellcode vaguely meeting the description with some, all, or more features, than described. Illustrate modern best practices.\n\nIn this case, you may try to meet some plausibly intended goals of the description, ignoring logical inconsistencies or other errors in the description. Details of the description are more guidelines or mere suggestions created without adequate planning, and thus may need to change significantly. Sloppy incorrect pseudocode may have been the basis for an incompetent technical writer creating the description by stating mere guesses about what the code does not do as if fact. Occasionally the description may be incomprehensible gibberish.\n\nPreamble or trailing context may be omitted by truncating to demonstrate the core technique.\n\n\nYou may treat this as an exercise and generate an essentially academic example.\n\nYou may roleplay, that is pretend,\nto generate a bash shellscript response from a segment of an especially large and sophisticated shell script,\nthat would be used with this prompt including the description, as a prompt/response pair,\nto teach an existing AI LLM model such as Llama 3.1 405b with already vast knowledge and logic from its original training,\nto know,\ncorrect bash shellcode commands and structures,\nfrom this subsequent fine-tuning using the segmented shell script as a vast set of more completely accurate correct examples.\n\n\nIn this case, as a fragment, lines of code needed before and after this code may be missing. All lines of code needed within the middle of the fragment should be present.\n\nIndividual bash commands must be useful, complete, accurate, perfected. Within the fragment, individual bash commands must exceed the highest practical standards for robustness, resilience, versatility, portability, compatibility, adaptability to changing inputs, consistent machine readable outputs, maintainability, etc.\n\nInputs to individual bash commands may be assembled programmatically as arrays and variables to reach such high standards.\n\n\n\nI will analyze the given bash shellcode segment as per your requirements.\n\n**Intended Functionality:**\nThe code appears to be a part of a larger bash script that provides a self-validation mechanism to ensure the script's integrity. It checks the checksum of the script's header and contents to ensure they match the predefined values. If the checksums don't match, the script exits with a non-zero status.\n\n**Logical Flow:**\n\n1. The script starts by adjusting the `PATH` environment variable to include `/usr/local/bin`, `/usr/bin`, and `/bin` if they are not already present.\n2. It then checks if the `ub_setScriptChecksum` variable is set and unsets it if it is.\n3. The script defines two functions: `_ub_cksum_special_derivativeScripts_header` and `_ub_cksum_special_derivativeScripts_contents`, which calculate the checksum of the script's header and contents, respectively.\n4. The script checks if the `ub_setScriptChecksum_disable` variable is set to `'true'` and skips the checksum validation if it is.\n5. If the checksum validation is not disabled, the script checks the checksum of the header and contents using the `_ub_cksum_special_derivativeScripts_header` and `_ub_cksum_special_derivativeScripts_contents` functions. If either checksum doesn't match the predefined value, the script exits with a non-zero status.\n6. The script defines two more functions: `_ub_cksum_special_derivativeScripts_write`, which updates the script's checksum values, and `_user_log-ub`, which is a debugging function that logs output to a file.\n\n**Input-Processing-Output:**\nThe script does not require any external inputs. It processes the script's own contents and outputs a non-zero status if the checksum validation fails. The `_user_log-ub` function takes input from `stdin` and logs it to a file.\n\n**Self-explanatory Naming Convention:**\nThe variable and function names are descriptive and follow a consistent naming convention. For example, `_ub_cksum_special_derivativeScripts_header` clearly indicates that it calculates the checksum of the script's header.\n\n**Commenting:**\nThe script has few comments, but they are informative and provide context where necessary. For example, the comment above the checksum validation section explains the purpose of the code.\n\n**Resilience:**\nThe script has a basic error-handling mechanism in place. If the checksum validation fails, the script exits with a non-zero status. However, there are no provisions for adapting to changes in the environment or inputs.\n\n**Robustness:**\nThe script uses standard bash utilities and does not rely on any specific program versions or unstable APIs. However, it does not have provisions for quick changes to accommodate unstable APIs or programs.\n\n**Versatility:**\nThe script avoids using special-purpose tools or programs and instead relies on general-purpose utilities like `sed` and `cksum`.\n\n**Portability:**\nThe script uses bash syntax and utilities that are widely available on most Unix-like systems, making it relatively portable.\n\n**Compatibility:**\nThe script does not explicitly check for or install dependencies. However, it does use widely available utilities, which reduces the likelihood of compatibility issues.\n\n**Adaptability:**\nThe script does not have provisions for automatically assembling parameters in arrays or adapting to different situations.\n\n**Consistent Machine Readability:**\nThe script's output is not designed to be machine-readable. However, the `_user_log-ub` function logs output in a simple format.\n\n**Maintainability:**\nThe script uses relatively simple bash constructs and avoids complex logic or numerical methods. This makes it easier to maintain and modify. However, the lack of comments and documentation may make it more challenging to understand the script's intent and functionality."},{"role":"assistant","content":"Here is a Bash script with extensive environment preparation and integrity checks, including a checksum validation mechanism.\n```bash\n#===== Segment 1: ubiquitous_bash: lines 1 to 78 =====\n#!/usr/bin/env bash\n\n[[ \"$PATH\" != *\"/usr/local/bin\"* ]] && [[ -e \"/usr/local/bin\" ]] && export PATH=/usr/local/bin:\"$PATH\"\n[[ \"$PATH\" != *\"/usr/bin\"* ]] && [[ -e \"/usr/bin\" ]] && export PATH=/usr/bin:\"$PATH\"\n[[ \"$PATH\" != *\"/bin:\"* ]] && [[ -e \"/bin\" ]] && export PATH=/bin:\"$PATH\"\n\nif [[ \"$ub_setScriptChecksum\" != \"\" ]]\nthen\n\texport ub_setScriptChecksum=\nfi\n\n_ub_cksum_special_derivativeScripts_header() {\n\tlocal currentFile_cksum\n\tif [[ \"$1\" == \"\" ]]\n\tthen\n\t\tcurrentFile_cksum=\"$0\"\n\telse\n\t\tcurrentFile_cksum=\"$1\"\n\tfi\n\t\n\thead -n 30 \"$currentFile_cksum\" | env CMD_ENV=xpg4 cksum | cut -f1 -d\\  | tr -dc '0-9'\n}\n_ub_cksum_special_derivativeScripts_contents() {\n\tlocal currentFile_cksum\n\tif [[ \"$1\" == \"\" ]]\n\tthen\n\t\tcurrentFile_cksum=\"$0\"\n\telse\n\t\tcurrentFile_cksum=\"$1\"\n\tfi\n\t\n\ttail -n +45 \"$currentFile_cksum\" | env CMD_ENV=xpg4 cksum | cut -f1 -d\\  | tr -dc '0-9'\n}\n##### CHECKSUM BOUNDARY - 30 lines\n\n#export ub_setScriptChecksum_disable='true'\n( [[ -e \"$0\".nck ]] || [[ \"${BASH_SOURCE[0]}\" != \"${0}\" ]] || [[ \"$1\" == '--profile' ]] || [[ \"$1\" == '--script' ]] || [[ \"$1\" == '--call' ]] || [[ \"$1\" == '--return' ]] || [[ \"$1\" == '--devenv' ]] || [[ \"$1\" == '--shell' ]] || [[ \"$1\" == '--bypass' ]] || [[ \"$1\" == '--parent' ]] || [[ \"$1\" == '--embed' ]] || [[ \"$1\" == '--compressed' ]] || [[ \"$0\" == \"/bin/bash\" ]] || [[ \"$0\" == \"-bash\" ]] || [[ \"$0\" == \"/usr/bin/bash\" ]] || [[ \"$0\" == \"bash\" ]] ) && export ub_setScriptChecksum_disable='true'\nexport ub_setScriptChecksum_header='2591634041'\nexport ub_setScriptChecksum_contents='2779813709'\n\n# CAUTION: Symlinks may cause problems. Disable this test for such cases if necessary.\n# WARNING: Performance may be crucial here.\n#[[ -e \"$0\" ]] && ! [[ -h \"$0\" ]] && [[ \"$ub_setScriptChecksum\" != \"\" ]]\nif [[ -e \"$0\" ]] && [[ \"$ub_setScriptChecksum_header\" != \"\" ]] && [[ \"$ub_setScriptChecksum_contents\" != \"\" ]] && [[ \"$ub_setScriptChecksum_disable\" != 'true' ]] #&& ! ( [[ -e \"$0\".nck ]] || [[ \"${BASH_SOURCE[0]}\" != \"${0}\" ]] || [[ \"$1\" == '--profile' ]] || [[ \"$1\" == '--script' ]] || [[ \"$1\" == '--call' ]] || [[ \"$1\" == '--return' ]] || [[ \"$1\" == '--devenv' ]] || [[ \"$1\" == '--shell' ]] || [[ \"$1\" == '--bypass' ]] || [[ \"$1\" == '--parent' ]] || [[ \"$1\" == '--embed' ]] || [[ \"$1\" == '--compressed' ]] || [[ \"$0\" == \"/bin/bash\" ]] || [[ \"$0\" == \"-bash\" ]] || [[ \"$0\" == \"/usr/bin/bash\" ]] || [[ \"$0\" == \"bash\" ]] )\nthen\n\t[[ $(_ub_cksum_special_derivativeScripts_header) != \"$ub_setScriptChecksum_header\" ]] && exit 1\n\t[[ $(_ub_cksum_special_derivativeScripts_contents) != \"$ub_setScriptChecksum_contents\" ]] && exit 1\nfi\n##### CHECKSUM BOUNDARY - 45 lines\n\n_ub_cksum_special_derivativeScripts_write() {\n\tlocal current_ub_setScriptChecksum_header\n\tlocal current_ub_setScriptChecksum_contents\n\n\tcurrent_ub_setScriptChecksum_header=$(_ub_cksum_special_derivativeScripts_header \"$1\")\n\tcurrent_ub_setScriptChecksum_contents=$(_ub_cksum_special_derivativeScripts_contents \"$1\")\n\n\tsed -i 's/'#'#'###uk4uPhB663kVcygT0q-UbiquitousBash-ScriptSelfModify-SetScriptChecksumHeader-UbiquitousBash-uk4uPhB663kVcygT0q#####'/'\"$current_ub_setScriptChecksum_header\"'/' \"$1\"\n\tsed -i 's/'#'#'###uk4uPhB663kVcygT0q-UbiquitousBash-ScriptSelfModify-SetScriptChecksumContents-UbiquitousBash-uk4uPhB663kVcygT0q#####'/'\"$current_ub_setScriptChecksum_contents\"'/' \"$1\"\n}\n\n\n#Universal debugging filesystem.\n_user_log-ub() {\n\t# DANGER Do NOT create automatically, or reference any existing directory!\n\t! [[ -d \"$HOME\"/.ubcore/userlog ]] && cat - > /dev/null 2>&1 && return 0\n\t\n\t#Terminal session may be used - the sessionid may be set through .bashrc/.ubcorerc .\n\tif [[ \"$sessionid\" != \"\" ]]\n\tthen\n\t\tcat - >> \"$HOME\"/.ubcore/userlog/u-\"$sessionid\".log\n\t\treturn 0\n\tfi\n\tcat - >> \"$HOME\"/.ubcore/userlog/u-undef.log\n\t\n\treturn 0\n}\n\n```\nThis code provides a comprehensive Bash script that incorporates various best practices for secure and reliable scripting. The script includes several key features that enhance its functionality and maintainability.\n\nThis code demonstrates the following best practices: \n\n*   It adjusts the system's PATH environment variable to ensure the inclusion of essential binary directories, promoting a more predictable and secure environment.\n*   The script checks for the integrity of the script file by comparing expected checksums for both the header and contents. This minimizes the risk of executing tampered or corrupted scripts.\n*   A user logging function is included, which allows for writing to a specific log file within the user's home directory. This log function takes into account session IDs if set, enhancing traceability and debugging capabilities.\n\nThis example includes the following features:\n\n*   Adaptive PATH modification based on directory existence.\n*   Checksum verification for script integrity.\n*   User-friendly log functionality with support for session IDs.\n\nNote that this script uses various external commands (`tail`, `head`, `sed`, `env`, `tr`, `cksum`, `cut`) which are widely available on most Unix-like systems. The script expects certain paths to exist, such as `/usr/local/bin`, `/usr/bin`, and `/bin`, which are typical on many systems but may need adjustments in specialized environments. Additionally, users may need to set a session ID (`sessionid`) in their `.bashrc` or `.ubcorerc` files to fully utilize the logging feature."}]}
```
























experiment-lora-1b.yml
```yaml
base_model: NousResearch/Llama-3.2-1B
# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name

load_in_8bit: false
load_in_4bit: false
strict: false

datasets:
  - path: teknium/GPT4-LLM-Cleaned
    type: alpaca
dataset_prepared_path: last_run_prepared
val_set_size: 0.1
output_dir: /outputs/lora-out

adapter: lora
lora_model_dir:

sequence_len: 2048
sample_packing: true
eval_sample_packing: true
pad_to_sequence_len: true

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_fan_in_fan_out:
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 2
micro_batch_size: 2
num_epochs: 1
optimizer: adamw_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention:
flash_attention: true

loss_watchdog_threshold: 5.0
loss_watchdog_patience: 3

warmup_steps: 10
evals_per_epoch: 4
saves_per_epoch: 1
debug:
deepspeed:
weight_decay: 0.0
fsdp:
fsdp_config:
special_tokens:
  pad_token: "<|end_of_text|>"

```






Please suggest commands, configuration, etc, to fine tune , quantize , and inference/test , Llama 3.1 8b , etc, using such compatible techiniques as axolotl , ollama , etc. Later fine-tuning is planned for Llama 3.1 405b using more powerful hardware from a runpod instance. Purely for low-VRAM experiments Llama 3.2 1b may be fine tuned as well. Dataset usability with such model fine-tuning services as OpenAI is also preferred.





