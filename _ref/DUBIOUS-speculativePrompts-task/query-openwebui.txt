
# tldr; Use the 'Condensed' prompt and remember to set 'Task Model' from the usual 'OpRt_.anthropic/claude-3.7-sonnet:thinking' useful for Merged Responses to 'OpRt_.meta-llama/llama-3.1-405b-instruct' and also use 'OpRt_.meta-llama/llama-3.1-405b-instruct' as the chat model if the best possible web search results are needed.


# BEWARE: OpenWebUI 'Local Models' and 'External Models' may require configuring to 'OpRt_.meta-llama/llama-3.1-405b-instruct' , etc, as 'OpRt_.anthropic/claude-3.7-sonnet:thinking' may or may not be used for web query generation if selected as the task model.
# https://github.com/open-webui/open-webui/issues/12022
# In practice, 'OpRt_.anthropic/claude-3.7-sonnet:thinking' specifically is unusually widely accurate enough that it suffers less from the narrowing of responses due to chain-of-reasoning, and so may sometimes be usable without always switching back to 'OpRt_.meta-llama/llama-3.1-405b-instruct' , the difference being searching something like 200 sites instead of 140 sites , and about half of the top 10 cited results explicitly mentioning a command, API call, etc, asked about, rather than all of the top 10 cited results. Cost of the resulting search is higher: ~$0.04 'OpRt_.anthropic/claude-3.7-sonnet:thinking' vs ~$0.01 'OpRt_.meta-llama/llama-3.1-405b-instruct' . Prompt tokens internally generated just from the web search results is of course significantly higher, but at a concerning limits: ~12000 'OpRt_.anthropic/claude-3.7-sonnet:thinking' vs ~15000 'OpRt_.meta-llama/llama-3.1-405b-instruct' , raising the possibility of misconfigured short context windows missing entire categories of search results entirely.

# Some AI LLM models have been known to follow the instructions of this prompt.
# OpRt_.meta-llama/llama-3.1-405b-instruct
# Llama-augment
# OpRt_.meta-llama/llama-4-scout:nitro


# Some AI LLM model outputs may be too narrowly focused for the purposes of this prompt.
# OpRt_.deepseek/deepseek-r1-distill-llama-70b
# OpRt_.meta-llama/llama-4-maverick:nitro

# Llama 3.1 (or better) non-reasoning LLM models are suggested.

# Some OpenWebUI Web Search parameters may have been tested. Larger numbers of search results may require an AI LLM model with a context-window and needle-in-haystack problem solving ability of >200k tokens (ie. 'Llama-4', etc) .
#
# Search Result Count 4
# Concurrent Requests 8
#
# Normally recommended for most searches, very rarely exceeds ~130k context .
# Search Result Count 10
# Concurrent Requests 20
#
# Search Result Count 18
# Concurrent Requests 20


# ATTRIBUTION-AI: et. al.



### Task:
Analyze the chat history to determine if search queries are needed.

Specifically generate separate relevant queries to retrieve official documentation (eg. official API documentation from the project's website), relevant source code (eg. relevant source files from the project's GitHub repository), official issue trackers (eg. GitHub Issues for the project, project website bugzilla subdomain), technical community sites (eg. Stack Overflow, Reddit), and separately, general information sources (eg. relevant Wikipedia articles).

Generate multiple specific queries for each information source category (aim for at least 3-5 per category).
Use specific terminology, error messages, and version numbers when applicable.
Combine keywords with search operators for more targeted results.
Focus queries explicitly on finding workarounds and bugfixes.
Always err on the side of generating more queries rather than fewer.

- Respond **EXCLUSIVELY** with a JSON object. Any form of extra commentary, explanation, or additional text is strictly prohibited.
- When generating search queries, respond in the format: { "queries": ["query1", "query2"] }, ensuring each query is distinct, concise, and relevant to the topic.
- Be concise and focused on composing high-quality search queries, avoiding unnecessary elaboration, commentary, or assumptions.

- At least 12-15 distinct search queries.

- If and only if it is entirely certain that no useful results can be retrieved by a search, return: { "queries": [] }.
- Err on the side of suggesting search queries if there is **any chance** they might provide useful or updated information.
- Today's date is: {{CURRENT_DATE}}.


### Output:
Strictly return in JSON format:
{
  "queries": ["query1", "query2"]
}

### Chat History:
<chat_history>
{{MESSAGES:END:6}}
</chat_history>








_ Condensed _

# ATTRIBUTION-AI: et. al. (partial)
#Please rewrite this prompt to better work within the limitations of smaller Large Language Models, as low as 8b parameters with little or no code review training.



Analyze the chat history to determine if search queries are needed. Create specific queries to retrieve from each of:
official documentation (project websites, API docs),
relevant source code files (GitHub repository files),
issue trackers (eg. GitHub Issues for the project, project website bugzilla subdomain),
technical community sites (eg. Stack Overflow, Reddit),
command references (official website API/ABI docs),
function call references (official website API/ABI docs),
general information (Wikipedia, tutorials).

Generate multiple specific queries for each information source category (aim for at least 2-3 per category).
Include specific terminology, error messages, and version numbers as applicable.
Use search operators (site:, filetype:sh, filetype:c, filetype:cpp, filetype:md, filetype:txt) when helpful.
Focus queries explicitly on finding workarounds, bugfixes, solutions, explanations, syntax, command reference pages, and function call reference pages.
Always err on the side of generating more queries rather than fewer.

- Respond **EXCLUSIVELY** with a JSON object. Any form of extra commentary, explanation, or additional text is strictly prohibited.
- When generating search queries, respond in the format: { "queries": ["query1", "query2"] }, ensuring each query is distinct, concise, and relevant to the topic.
- Be concise and focused on composing high-quality search queries, avoiding unnecessary elaboration, commentary, or assumptions.

- At least 8-11 distinct search queries.

- Err on the side of suggesting search queries if there is **any chance** they might provide useful or updated information.


### Output:
Strictly return in JSON format:
{
  "queries": ["query1", "query2"]
}

### Chat History:
<chat_history>
{{MESSAGES:END:6}}
</chat_history>










_ EXAMPLE _
EXAMPLE orig (from  https://github.com/open-webui/open-webui/blob/main/backend/open_webui/config.py  'DEFAULT_QUERY_GENERATION_PROMPT_TEMPLATE')

### Task:
Analyze the chat history to determine the necessity of generating search queries, in the given language. By default, **prioritize generating 1-3 broad and relevant search queries** unless it is absolutely certain that no additional information is required. The aim is to retrieve comprehensive, updated, and valuable information even with minimal uncertainty. If no search is unequivocally needed, return an empty list.

### Guidelines:
- Respond **EXCLUSIVELY** with a JSON object. Any form of extra commentary, explanation, or additional text is strictly prohibited.
- When generating search queries, respond in the format: { "queries": ["query1", "query2"] }, ensuring each query is distinct, concise, and relevant to the topic.
- If and only if it is entirely certain that no useful results can be retrieved by a search, return: { "queries": [] }.
- Err on the side of suggesting search queries if there is **any chance** they might provide useful or updated information.
- Be concise and focused on composing high-quality search queries, avoiding unnecessary elaboration, commentary, or assumptions.
- Today's date is: {{CURRENT_DATE}}.
- Always prioritize providing actionable and broad queries that maximize informational coverage.

### Output:
Strictly return in JSON format:
{
  "queries": ["query1", "query2"]
}

### Chat History:
<chat_history>
{{MESSAGES:END:6}}
</chat_history>
