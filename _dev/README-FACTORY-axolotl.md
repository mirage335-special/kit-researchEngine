Essential information to operate 'Llama-tech' factory .
```
[___quick]'/project/zFactory/'Llama-tech
```



# Scrap - Install axolotl





# Scrap - Inference



# Scrap - Quantization



# Scrap - Quantization - push HuggingFace






# Scrap - Example



# Scratch - Example

"Llama-3.1" "See axolotl config" site:huggingface.co
https://huggingface.co/axolotl-ai-co/finetome-llama-3.1-70b
 'finetome'
https://huggingface.co/minionai/llama_3.1_70b_cove_prod_81424_amazon_filt_merged
https://huggingface.co/pbevan11/llama-3.1-8b-ocr-correction
https://huggingface.co/ericflo/Llama-3.1-SyntheticPython-405B-Base-LoRA
https://huggingface.co/SE6446/Llama-3.1-SuperNova-Lite-Reflection-V1.0
 'model appears to perform adequatel'
 'Loss: 0.6365'
 'must use the tokenizer provided with the model as the COT tokens are unique special tokens'
 'should work on most inference engines that can run llama 3.1'
  '2.7211	0.0049	1	1.4048
   0.6381	0.5	103	0.6583
   0.4985	1.0049	206	0.6320
   0.4992	1.5049	309	0.6365'
https://huggingface.co/femT-data/llama-3.1-8B-instruct-GNER
https://huggingface.co/suayptalha/EmojiLlama-3.1-8B
https://huggingface.co/winglian/llama-3.1-8b-math-r1
https://huggingface.co/jplhughes2/1a_meta-llama-Llama-3.1-405B-Instruct-fsdp
https://huggingface.co/VoyagerYuan/Xillama-3.1-405B-Instruct-BNB-NF4-BF16-Writer
https://huggingface.co/axolotl-ai-co/numina-70b-ep3-lr1e-5-sft-lora
https://huggingface.co/anthracite-org/magnum-v2-4b


















